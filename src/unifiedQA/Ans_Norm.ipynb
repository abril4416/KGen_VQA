{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c149c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "OK_PATH='/Data_Storage/Rui_Data_Space/VQA/OK-VQA'\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d5bda01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle as pkl\n",
    "\n",
    "def load_pkl(path):\n",
    "    data=pkl.load(open(path,'rb'))\n",
    "    return data\n",
    "\n",
    "def load_json(path):\n",
    "    data=json.load(open(path,'r'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3f93a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f434197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5046 5046\n"
     ]
    }
   ],
   "source": [
    "val_anno=load_json(os.path.join(OK_PATH,'mscoco_val2014_annotations.json'))['annotations']\n",
    "ans_dict={}\n",
    "for row in val_anno:\n",
    "    ques_id=str(row['question_id'])\n",
    "    answers=defaultdict(int)\n",
    "    for info in row['answers']:\n",
    "        answers[info['answer']]+=1\n",
    "    #ans={ans:answers[ans]/sum(answers.values()) for ans in answers.keys()}\n",
    "    ans_dict[ques_id]=answers\n",
    "print (len(val_anno),len(ans_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dbb0aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5046\n"
     ]
    }
   ],
   "source": [
    "vqa_all=load_json(os.path.join(OK_PATH,'OpenEnded_mscoco_val2014_questions.json'))['questions']\n",
    "print (len(vqa_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81b52e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_row={}\n",
    "for row in vqa_all:\n",
    "    id_to_row[str(row['question_id'])]=row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e76b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(occurences):\n",
    "    return min(1.0,occurences/3.0)\n",
    "\n",
    "import inflect\n",
    "single = inflect.engine()\n",
    "\n",
    "def norm_ans(ans):\n",
    "    if ans.endswith(',') or ans.endswith('.'):\n",
    "        ans=ans[:-1]\n",
    "    if ans.startswith('a '):\n",
    "        ans=ans[2:]\n",
    "    if ',' in ans:\n",
    "        ans=ans.split(',')[0]\n",
    "    if len(ans.split(' '))==1 and ans not in ['grass','glass','bus']:\n",
    "        norm_ans=single.singular_noun(ans)\n",
    "        if norm_ans !=False:\n",
    "            ans=norm_ans\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f50c22b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader import VERB\n",
    "\n",
    "_MANUAL_MATCHES = {\n",
    "    \"police\": \"police\",\n",
    "    \"las\": \"las\",\n",
    "    \"vegas\": \"vegas\",\n",
    "    \"yes\": \"yes\",\n",
    "    \"jeans\": \"jean\",\n",
    "    \"hell's\": \"hell\",\n",
    "    \"domino's\": \"domino\",\n",
    "    \"morning\": \"morn\",\n",
    "    \"clothes\": \"cloth\",\n",
    "    \"are\": \"are\",\n",
    "    \"riding\": \"ride\",\n",
    "    \"leaves\": \"leaf\",\n",
    "    \"dangerous\": \"danger\",\n",
    "    \"clothing\": \"cloth\",\n",
    "    \"texting\": \"text\",\n",
    "    \"kiting\": \"kite\",\n",
    "    \"firefighters\": \"firefight\",\n",
    "    \"ties\": \"tie\",\n",
    "    \"married\": \"married\",\n",
    "    \"teething\": \"teeth\",\n",
    "    \"gloves\": \"glove\",\n",
    "    \"tennis\": \"tennis\",\n",
    "    \"dining\": \"dine\",\n",
    "    \"directions\": \"direct\",\n",
    "    \"waves\": \"wave\",\n",
    "    \"christmas\": \"christmas\",\n",
    "    \"drives\": \"drive\",\n",
    "    \"pudding\": \"pud\",\n",
    "    \"coding\": \"code\",\n",
    "    \"plating\": \"plate\",\n",
    "    \"quantas\": \"quanta\",\n",
    "    \"hornes\": \"horn\",\n",
    "    \"graves\": \"grave\",\n",
    "    \"mating\": \"mate\",\n",
    "    \"paned\": \"pane\",\n",
    "    \"alertness\": \"alert\",\n",
    "    \"sunbathing\": \"sunbath\",\n",
    "    \"tenning\": \"ten\",\n",
    "    \"wetness\": \"wet\",\n",
    "    \"urinating\": \"urine\",\n",
    "    \"sickness\": \"sick\",\n",
    "    \"braves\": \"brave\",\n",
    "    \"firefighting\": \"firefight\",\n",
    "    \"lenses\": \"lens\",\n",
    "    \"reflections\": \"reflect\",\n",
    "    \"backpackers\": \"backpack\",\n",
    "    \"eatting\": \"eat\",\n",
    "    \"designers\": \"design\",\n",
    "    \"curiousity\": \"curious\",\n",
    "    \"playfulness\": \"play\",\n",
    "    \"blindness\": \"blind\",\n",
    "    \"hawke\": \"hawk\",\n",
    "    \"tomatoe\": \"tomato\",\n",
    "    \"rodeoing\": \"rodeo\",\n",
    "    \"brightness\": \"bright\",\n",
    "    \"circuses\": \"circus\",\n",
    "    \"skateboarders\": \"skateboard\",\n",
    "    \"staring\": \"stare\",\n",
    "    \"electronics\": \"electron\",\n",
    "    \"electicity\": \"elect\",\n",
    "    \"mountainous\": \"mountain\",\n",
    "    \"socializing\": \"social\",\n",
    "    \"hamburgers\": \"hamburg\",\n",
    "    \"caves\": \"cave\",\n",
    "    \"transitions\": \"transit\",\n",
    "    \"wading\": \"wade\",\n",
    "    \"creame\": \"cream\",\n",
    "    \"toileting\": \"toilet\",\n",
    "    \"sautee\": \"saute\",\n",
    "    \"buildings\": \"build\",\n",
    "    \"belongings\": \"belong\",\n",
    "    \"stockings\": \"stock\",\n",
    "    \"walle\": \"wall\",\n",
    "    \"cumulis\": \"cumuli\",\n",
    "    \"travelers\": \"travel\",\n",
    "    \"conducter\": \"conduct\",\n",
    "    \"browsing\": \"brows\",\n",
    "    \"pooping\": \"poop\",\n",
    "    \"haircutting\": \"haircut\",\n",
    "    \"toppings\": \"top\",\n",
    "    \"hearding\": \"heard\",\n",
    "    \"sunblocker\": \"sunblock\",\n",
    "    \"bases\": \"base\",\n",
    "    \"markings\": \"mark\",\n",
    "    \"mopeds\": \"mope\",\n",
    "    \"kindergartener\": \"kindergarten\",\n",
    "    \"pies\": \"pie\",\n",
    "    \"scrapbooking\": \"scrapbook\",\n",
    "    \"couponing\": \"coupon\",\n",
    "    \"meetings\": \"meet\",\n",
    "    \"elevators\": \"elev\",\n",
    "    \"lowes\": \"low\",\n",
    "    \"men's\": \"men\",\n",
    "    \"childrens\": \"children\",\n",
    "    \"shelves\": \"shelve\",\n",
    "    \"paintings\": \"paint\",\n",
    "    \"raines\": \"rain\",\n",
    "    \"paring\": \"pare\",\n",
    "    \"expressions\": \"express\",\n",
    "    \"routes\": \"rout\",\n",
    "    \"pease\": \"peas\",\n",
    "    \"vastness\": \"vast\",\n",
    "    \"awning\": \"awn\",\n",
    "    \"boy's\": \"boy\",\n",
    "    \"drunkenness\": \"drunken\",\n",
    "    \"teasing\": \"teas\",\n",
    "    \"conferences\": \"confer\",\n",
    "    \"ripeness\": \"ripe\",\n",
    "    \"suspenders\": \"suspend\",\n",
    "    \"earnings\": \"earn\",\n",
    "    \"reporters\": \"report\",\n",
    "    \"kid's\": \"kid\",\n",
    "    \"containers\": \"contain\",\n",
    "    \"corgie\": \"corgi\",\n",
    "    \"porche\": \"porch\",\n",
    "    \"microwaves\": \"microwave\",\n",
    "    \"batter's\": \"batter\",\n",
    "    \"sadness\": \"sad\",\n",
    "    \"apartments\": \"apart\",\n",
    "    \"oxygenize\": \"oxygen\",\n",
    "    \"striping\": \"stripe\",\n",
    "    \"purring\": \"pure\",\n",
    "    \"professionals\": \"profession\",\n",
    "    \"piping\": \"pipe\",\n",
    "    \"farmer's\": \"farmer\",\n",
    "    \"potatoe\": \"potato\",\n",
    "    \"emirates\": \"emir\",\n",
    "    \"womens\": \"women\",\n",
    "    \"veteran's\": \"veteran\",\n",
    "    \"wilderness\": \"wilder\",\n",
    "    \"propellers\": \"propel\",\n",
    "    \"alpes\": \"alp\",\n",
    "    \"charioteering\": \"chariot\",\n",
    "    \"swining\": \"swine\",\n",
    "    \"illness\": \"ill\",\n",
    "    \"crepte\": \"crept\",\n",
    "    \"adhesives\": \"adhesive\",\n",
    "    \"regent's\": \"regent\",\n",
    "    \"decorations\": \"decor\",\n",
    "    \"rabbies\": \"rabbi\",\n",
    "    \"overseas\": \"oversea\",\n",
    "    \"travellers\": \"travel\",\n",
    "    \"casings\": \"case\",\n",
    "    \"smugness\": \"smug\",\n",
    "    \"doves\": \"dove\",\n",
    "    \"nationals\": \"nation\",\n",
    "    \"mustange\": \"mustang\",\n",
    "    \"ringe\": \"ring\",\n",
    "    \"gondoliere\": \"gondolier\",\n",
    "    \"vacationing\": \"vacate\",\n",
    "    \"reminders\": \"remind\",\n",
    "    \"baldness\": \"bald\",\n",
    "    \"settings\": \"set\",\n",
    "    \"glaced\": \"glace\",\n",
    "    \"coniferous\": \"conifer\",\n",
    "    \"revelations\": \"revel\",\n",
    "    \"personals\": \"person\",\n",
    "    \"daughter's\": \"daughter\",\n",
    "    \"badness\": \"bad\",\n",
    "    \"projections\": \"project\",\n",
    "    \"polarizing\": \"polar\",\n",
    "    \"vandalizers\": \"vandal\",\n",
    "    \"minerals\": \"miner\",\n",
    "    \"protesters\": \"protest\",\n",
    "    \"controllers\": \"control\",\n",
    "    \"weddings\": \"wed\",\n",
    "    \"sometimes\": \"sometime\",\n",
    "    \"earing\": \"ear\",\n",
    "}\n",
    "_wordnet_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "import inflection\n",
    "def stem(input_string):\n",
    "    word_and_pos = nltk.pos_tag(nltk.tokenize.word_tokenize(input_string))\n",
    "    stemmed_words = []\n",
    "    for w, p in word_and_pos:\n",
    "        if w in _MANUAL_MATCHES:\n",
    "            w = _MANUAL_MATCHES[w]\n",
    "        elif w.endswith(\"ing\"):\n",
    "            w = _wordnet_lemmatizer.lemmatize(w, VERB)\n",
    "        elif p.startswith(\"NNS\") or p.startswith(\"NNPS\"):\n",
    "            w = inflection.singularize(w)\n",
    "        stemmed_words.append(w)\n",
    "    return \" \".join(stemmed_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c47c5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.13489232395321\n",
      "5046\n"
     ]
    }
   ],
   "source": [
    "vis=0\n",
    "acc=0.0\n",
    "pred_file=load_pkl('pnp_ablations_65_10.pkl')\n",
    "for ques_id in pred_file:\n",
    "    pred=pred_file[ques_id]\n",
    "    #pred=norm_ans(pred)\n",
    "    if pred in ans_dict[ques_id]:\n",
    "        acc+=get_score(ans_dict[ques_id][pred])\n",
    "    vis+=1\n",
    "print (acc*100.0/vis)\n",
    "print (vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "345721e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.53032104637353\n",
      "5046\n"
     ]
    }
   ],
   "source": [
    "vis=0\n",
    "acc=0.0\n",
    "for ques_id in pred_file:\n",
    "    pred=pred_file[ques_id]\n",
    "    pred=norm_ans(pred)\n",
    "    pred=stem(pred)\n",
    "    if pred in ans_dict[ques_id]:\n",
    "        acc+=get_score(ans_dict[ques_id][pred])\n",
    "    vis+=1\n",
    "print (acc*100.0/vis)\n",
    "print (vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db81cf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.92693882943605\n",
      "5046\n",
      "39.63535473642512\n",
      "5046\n"
     ]
    }
   ],
   "source": [
    "vis=0\n",
    "acc=0.0\n",
    "pred_file=load_pkl('pnp_ablations_65_0.pkl')\n",
    "for ques_id in pred_file:\n",
    "    pred=pred_file[ques_id]\n",
    "    #pred=norm_ans(pred)\n",
    "    if pred in ans_dict[ques_id]:\n",
    "        acc+=get_score(ans_dict[ques_id][pred])\n",
    "    vis+=1\n",
    "print (acc*100.0/vis)\n",
    "print (vis)\n",
    "\n",
    "vis=0\n",
    "acc=0.0\n",
    "for ques_id in pred_file:\n",
    "    pred=pred_file[ques_id]\n",
    "    pred=norm_ans(pred)\n",
    "    pred=stem(pred)\n",
    "    if pred in ans_dict[ques_id]:\n",
    "        acc+=get_score(ans_dict[ques_id][pred])\n",
    "    vis+=1\n",
    "print (acc*100.0/vis)\n",
    "print (vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aea1858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff7240a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
