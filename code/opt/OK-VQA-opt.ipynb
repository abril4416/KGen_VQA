{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71bd2f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='11,13,14,15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5d7b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "OK_PATH='/Data_Storage/Rui_Data_Space/VQA/OK-VQA'\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "178cc59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle as pkl\n",
    "\n",
    "def load_pkl(path):\n",
    "    data=pkl.load(open(path,'rb'))\n",
    "    return data\n",
    "\n",
    "def load_json(path):\n",
    "    data=json.load(open(path,'r'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b727311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a28d062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5046 5046\n"
     ]
    }
   ],
   "source": [
    "val_anno=load_json(os.path.join(OK_PATH,'mscoco_val2014_annotations.json'))['annotations']\n",
    "ans_dict={}\n",
    "for row in val_anno:\n",
    "    ques_id=str(row['question_id'])\n",
    "    answers=defaultdict(int)\n",
    "    for info in row['answers']:\n",
    "        answers[info['answer']]+=1\n",
    "    #ans={ans:answers[ans]/sum(answers.values()) for ans in answers.keys()}\n",
    "    ans_dict[ques_id]=answers\n",
    "print (len(val_anno),len(ans_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91993806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5046\n"
     ]
    }
   ],
   "source": [
    "vqa_all=load_json(os.path.join(OK_PATH,'OpenEnded_mscoco_val2014_questions.json'))['questions']\n",
    "print (len(vqa_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "136bcee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-23 12:48:56,614] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be3e836f8f704f7da9cdba6910d4283e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-13b\", \n",
    "                                             torch_dtype=torch.float16, \n",
    "                                             device_map=\"auto\")\n",
    "\n",
    "# the fast tokenizer currently does not work correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f230f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-13b\", \n",
    "                                          padding_side='left',\n",
    "                                          use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "221dae2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5046\n"
     ]
    }
   ],
   "source": [
    "syn_qa_pairs=load_pkl('../OK_VQA/ok_vqa_qa_img2llm.pkl')\n",
    "print(len(syn_qa_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62216456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(occurences):\n",
    "    return min(1.0,occurences/3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b05b88f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect\n",
    "single = inflect.engine()\n",
    "\n",
    "def norm_ans(ans):\n",
    "    if ans.endswith(',') or ans.endswith('.'):\n",
    "        ans=ans[:-1]\n",
    "    if ans.startswith('a '):\n",
    "        ans=ans[2:]\n",
    "    if ',' in ans:\n",
    "        ans=ans.split(',')[0]\n",
    "    if len(ans.split(' '))==1 and ans not in ['grass','glass','bus']:\n",
    "        norm_ans=single.singular_noun(ans)\n",
    "        if norm_ans !=False:\n",
    "            ans=norm_ans\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "572e205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DEMO=15\n",
    "NUM_KB=5\n",
    "NUM_CAP=50\n",
    "\n",
    "head='Please answer the question accordint to the above context and knowledge with as few words as possible.\\n'\n",
    "def prompt_construction_kb(entry,ques,kb,contexts):\n",
    "    questions=entry['questions']\n",
    "    answers=entry['answers']\n",
    "    captions=entry['captions']\n",
    "    random.shuffle(captions)\n",
    "    all_texts=[head+'\\n']\n",
    "    all_texts.append('Contexts:'+' . '.join(captions[:NUM_DEMO+NUM_KB])+'\\n')\n",
    "    all_texts.append('Knowledge:'+' '.join(kb[:NUM_KB])+'\\n')\n",
    "    for i,q in enumerate(questions[:NUM_DEMO]):\n",
    "        all_texts.append('Question:'+q+'\\nAnswer:'+answers[i])\n",
    "    all_texts.append('Question:'+\\\n",
    "                     ques+\\\n",
    "                     '\\nAnswer:')\n",
    "    all_texts='\\n'.join(all_texts)\n",
    "    return all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d6ef13b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 5120, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 5120)\n",
       "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (12): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (13): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (14): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (15): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (16): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (17): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (18): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (19): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (20): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (21): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (22): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (23): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (24): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (25): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (26): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (27): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (28): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (29): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (30): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (31): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (32): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (33): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (34): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (35): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (36): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (37): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (38): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (39): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0c7aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d03e9e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already finished: 200\n",
      "\tAcc: 35.99999999999999\n",
      "Already finished: 400\n",
      "\tAcc: 40.583333333333314\n",
      "Already finished: 600\n",
      "\tAcc: 39.055555555555515\n",
      "Already finished: 800\n",
      "\tAcc: 38.375\n",
      "Already finished: 1000\n",
      "\tAcc: 37.63333333333338\n",
      "Already finished: 1200\n",
      "\tAcc: 36.19444444444451\n",
      "Already finished: 1400\n",
      "\tAcc: 36.071428571428655\n",
      "Already finished: 1600\n",
      "\tAcc: 36.62500000000003\n",
      "Already finished: 1800\n",
      "\tAcc: 37.07407407407406\n",
      "Already finished: 2000\n",
      "\tAcc: 36.89999999999995\n",
      "Already finished: 2200\n",
      "\tAcc: 37.19696969696961\n",
      "Already finished: 2400\n",
      "\tAcc: 37.3472222222221\n",
      "Already finished: 2600\n",
      "\tAcc: 37.34615384615371\n",
      "Already finished: 2800\n",
      "\tAcc: 37.33333333333321\n",
      "Already finished: 3000\n",
      "\tAcc: 37.17777777777771\n",
      "Already finished: 3200\n",
      "\tAcc: 37.427083333333314\n",
      "Already finished: 3400\n",
      "\tAcc: 37.13725490196082\n",
      "Already finished: 3600\n",
      "\tAcc: 36.907407407407476\n",
      "Already finished: 3800\n",
      "\tAcc: 37.24561403508784\n",
      "Already finished: 4000\n",
      "\tAcc: 37.22500000000015\n",
      "Already finished: 4200\n",
      "\tAcc: 37.38888888888907\n",
      "Already finished: 4400\n",
      "\tAcc: 37.46212121212142\n",
      "Already finished: 4600\n",
      "\tAcc: 37.659420289855305\n",
      "Already finished: 4800\n",
      "\tAcc: 37.69444444444469\n",
      "Already finished: 5000\n",
      "\tAcc: 37.40000000000026\n",
      "37.30347469943216\n"
     ]
    }
   ],
   "source": [
    "#13b ablations\n",
    "NUM_DEMO=15\n",
    "NUM_KB=0\n",
    "NUM_CAP=30\n",
    "\n",
    "head='Please answer the question accordint to the above context and knowledge with as few words as possible.\\n'\n",
    "def prompt_construction_kb(entry,ques,kb,contexts):\n",
    "    questions=entry['questions']\n",
    "    answers=entry['answers']\n",
    "    captions=entry['captions']\n",
    "    random.shuffle(captions)\n",
    "    all_texts=[head+'\\n']\n",
    "    all_texts.append('Contexts:'+' . '.join(captions[:NUM_DEMO])+'\\n')\n",
    "    #all_texts.append('Knowledge:'+' '.join(kb[:NUM_KB])+'\\n')\n",
    "    for i,q in enumerate(questions[:NUM_DEMO]):\n",
    "        all_texts.append('Question:'+q+'\\nAnswer:'+answers[i])\n",
    "    all_texts.append('Question:'+\\\n",
    "                     ques+\\\n",
    "                     '\\nAnswer:')\n",
    "    all_texts='\\n'.join(all_texts)\n",
    "    return all_texts\n",
    "\n",
    "#kb=0,num_cap=30,num_demo=15\n",
    "\n",
    "vis=0\n",
    "random.shuffle(vqa_all)\n",
    "acc=0.0\n",
    "for k,row in enumerate(vqa_all):\n",
    "    if k>0 and k%200==0:\n",
    "        print ('Already finished:',vis)\n",
    "        print ('\\tAcc:',acc*100.0/vis)\n",
    "    ques_id=str(row['question_id'])\n",
    "    ques=row['question']\n",
    "    answers=ans_dict[ques_id]\n",
    "    entry=syn_qa_pairs[ques_id]\n",
    "    kb=load_json(os.path.join('../OK_VQA/cluster_generated_kb',\n",
    "                              ques_id+'.json'))\n",
    "    captions=load_json('../OK_VQA/large_captions/'+ques_id+'.json')\n",
    "    prompt=prompt_construction_kb(entry,ques,kb,captions)\n",
    "    #print (prompt)\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        output=model.generate(inputs['input_ids'].to(0), max_length=2000,\n",
    "                              length_penalty=-1,\n",
    "                              min_length=2,\n",
    "                              eos_token_id=50118)\n",
    "    pred=tokenizer.decode(output[0].tolist()).split(':')[-1].strip().lower()\n",
    "    pred=norm_ans(pred)\n",
    "    if pred in answers:\n",
    "        acc+=get_score(answers[pred])\n",
    "    #print('\\t',ques,ans_dict[ques_id])\n",
    "    #print (pred)\n",
    "    vis+=1\n",
    "print (acc*100.0/vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be34f369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already finished: 200\n",
      "\tAcc: 40.333333333333336\n",
      "Already finished: 400\n",
      "\tAcc: 40.58333333333333\n",
      "Already finished: 600\n",
      "\tAcc: 40.722222222222186\n",
      "Already finished: 800\n",
      "\tAcc: 41.0\n",
      "Already finished: 1000\n",
      "\tAcc: 41.200000000000024\n",
      "Already finished: 1200\n",
      "\tAcc: 40.72222222222227\n",
      "Already finished: 1400\n",
      "\tAcc: 39.80952380952384\n",
      "Already finished: 1600\n",
      "\tAcc: 40.45833333333331\n",
      "Already finished: 1800\n",
      "\tAcc: 40.57407407407402\n",
      "Already finished: 2000\n",
      "\tAcc: 40.84999999999992\n",
      "Already finished: 2200\n",
      "\tAcc: 40.83333333333322\n",
      "Already finished: 2400\n",
      "\tAcc: 40.62499999999988\n",
      "Already finished: 2600\n",
      "\tAcc: 40.51282051282042\n",
      "Already finished: 2800\n",
      "\tAcc: 40.34523809523805\n",
      "Already finished: 3000\n",
      "\tAcc: 40.5888888888889\n",
      "Already finished: 3200\n",
      "\tAcc: 40.291666666666714\n",
      "Already finished: 3400\n",
      "\tAcc: 40.12745098039225\n",
      "Already finished: 3600\n",
      "\tAcc: 39.62037037037048\n",
      "Already finished: 3800\n",
      "\tAcc: 39.710526315789615\n",
      "Already finished: 4000\n",
      "\tAcc: 39.90833333333349\n",
      "Already finished: 4200\n",
      "\tAcc: 40.23015873015891\n",
      "Already finished: 4400\n",
      "\tAcc: 40.37121212121234\n",
      "Already finished: 4600\n",
      "\tAcc: 40.4130434782611\n",
      "Already finished: 4800\n",
      "\tAcc: 40.22916666666692\n",
      "Already finished: 5000\n",
      "\tAcc: 40.24666666666694\n",
      "40.23649094992761\n"
     ]
    }
   ],
   "source": [
    "#13b ablations\n",
    "NUM_DEMO=15\n",
    "NUM_KB=5\n",
    "NUM_CAP=30\n",
    "\n",
    "head='Please answer the question accordint to the above context and knowledge with as few words as possible.\\n'\n",
    "def prompt_construction_kb(entry,ques,kb,contexts):\n",
    "    questions=entry['questions']\n",
    "    answers=entry['answers']\n",
    "    captions=entry['captions']\n",
    "    random.shuffle(captions)\n",
    "    all_texts=[head+'\\n']\n",
    "    all_texts.append('Contexts:'+' . '.join(captions[:NUM_DEMO])+'\\n')\n",
    "    all_texts.append('Knowledge:'+' '.join(kb[:NUM_KB])+'\\n')\n",
    "    for i,q in enumerate(questions[:NUM_DEMO]):\n",
    "        all_texts.append('Question:'+q+'\\nAnswer:'+answers[i])\n",
    "    all_texts.append('Question:'+\\\n",
    "                     ques+\\\n",
    "                     '\\nAnswer:')\n",
    "    all_texts='\\n'.join(all_texts)\n",
    "    return all_texts\n",
    "\n",
    "#kb=0,num_cap=30,num_demo=15\n",
    "\n",
    "vis=0\n",
    "random.shuffle(vqa_all)\n",
    "acc=0.0\n",
    "for k,row in enumerate(vqa_all):\n",
    "    if k>0 and k%200==0:\n",
    "        print ('Already finished:',vis)\n",
    "        print ('\\tAcc:',acc*100.0/vis)\n",
    "    ques_id=str(row['question_id'])\n",
    "    ques=row['question']\n",
    "    answers=ans_dict[ques_id]\n",
    "    entry=syn_qa_pairs[ques_id]\n",
    "    kb=load_json(os.path.join('../OK_VQA/cluster_generated_kb',\n",
    "                              ques_id+'.json'))\n",
    "    captions=load_json('../OK_VQA/large_captions/'+ques_id+'.json')\n",
    "    prompt=prompt_construction_kb(entry,ques,kb,captions)\n",
    "    #print (prompt)\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        output=model.generate(inputs['input_ids'].to(0), max_length=2000,\n",
    "                              length_penalty=-1,\n",
    "                              min_length=2,\n",
    "                              eos_token_id=50118)\n",
    "    pred=tokenizer.decode(output[0].tolist()).split(':')[-1].strip().lower()\n",
    "    pred=norm_ans(pred)\n",
    "    if pred in answers:\n",
    "        acc+=get_score(answers[pred])\n",
    "    #print('\\t',ques,ans_dict[ques_id])\n",
    "    #print (pred)\n",
    "    vis+=1\n",
    "print (acc*100.0/vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0054313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f24e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2641e43e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5d9c55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
